{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shelunts/thesis/TabularSemantingParsing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_inputs.parse_sql_one import get_schemas_from_json, Schema\n",
    "from preprocess_inputs.process_sql import get_sql\n",
    "from preprocess_inputs.get_tables import convert_fk_index, dump_db_json_schema\n",
    "import json\n",
    "import sqlite3\n",
    "from os.path import isfile, isdir, join, split, exists, splitext\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from src.eval.spider.evaluate import Evaluator\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "DATA_DIR = ROOT_DIR + '/data/spider/database/isrecon/isrecon.sqlite'\n",
    "SCHEMA_DIR = ROOT_DIR + '/data/spider/tables.json' \n",
    "SQL_DIR = ROOT_DIR + '/data/isrecon/isrecon/test_sql_pairs.sql'\n",
    "DEV_DIR = ROOT_DIR + '/data/spider/dev.json'\n",
    "PREDS_DIR1 = ROOT_DIR + '/model/isrecon_predictions/preds_isrecon_model1_final.txt'\n",
    "PREDS_DIR2 = ROOT_DIR + '/model/isrecon_predictions/preds_isrecon_model2_final.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: ISRECON predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the connection to the ISRECON DB\n",
    "connection = sqlite3.connect(DATA_DIR)\n",
    "crsr = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dsr_count',), ('papers',), ('authors',), ('citations',), ('reference_citations',), ('papers_keywords',)]\n"
     ]
    }
   ],
   "source": [
    "crsr.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(crsr.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SQL_DIR, encoding='utf8') as f:\n",
    "    l = [line.rstrip('\\n') for line in f]\n",
    "    queries = [v for i, v in enumerate(l) if i % 2 != 0]\n",
    "predictions1 = open(PREDS_DIR1, 'r').read().split('\\n')\n",
    "predictions2 = open(PREDS_DIR2, 'r').read().split('\\n')\n",
    "# predictions1 = open('/home/shelunts/thesis/TabularSemantingParsing/model/model2_recon/predictions.txt', 'r').read().split('\\n')\n",
    "# predictions2 = open('/home/shelunts/thesis/TabularSemantingParsing/model/spider.bridge.lstm.meta.ts.ppl-0.85.2.dn.eo.feat.bert-large-uncased.xavier-1024-400-400-8-2-0.0005-inv-sqr-0.0005-4000-6e-05-inv-sqr-3e-05-4000-0.3-0.3-0.0-0.0-1-8-0.0-0.0-res-0.2-0.0-ff-0.4-0.0/predictions_isrecon_model2.txt', 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executing results of ground truth queries and appending to the list\n",
    "results_truths = [crsr.execute(i).fetchall() for i in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executing results of predictions of model 1 and appending to the list\n",
    "results_predictions1 = []\n",
    "for i in predictions1:\n",
    "    try:\n",
    "        prediction1 = crsr.execute(i).fetchall()\n",
    "        results_predictions1.append(prediction1)\n",
    "    except:\n",
    "        results_predictions1.append(\"Error\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executing results of predictions of model 2 and appending to the list\n",
    "results_predictions2 = []\n",
    "for i in predictions2:\n",
    "    try:\n",
    "        prediction2 = crsr.execute(i).fetchall()\n",
    "        results_predictions2.append(prediction2)\n",
    "    except:\n",
    "        results_predictions2.append(\"Error\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: checking if prediction is correct by comparing query output of ground truth queries vs predictions\n",
    "accurate_predictions_indices1 = []\n",
    "accurate_predictions_count1 = 0\n",
    "prediction_status1 = []\n",
    "idx = 0\n",
    "for i in results_truths:\n",
    "            if i == results_predictions1[idx]:\n",
    "                accurate_predictions_indices1.append(idx)\n",
    "                prediction_status1.append(True)\n",
    "                accurate_predictions_count1 +=1\n",
    "            elif results_predictions1[idx] == 'Error':\n",
    "                prediction_status1.append('Error')\n",
    "            else:\n",
    "                prediction_status1.append(False)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: checking if prediction is correct by comparing query output of ground truth queries vs predictions\n",
    "accurate_predictions_indices2 = []\n",
    "accurate_predictions_count2 = 0\n",
    "prediction_status2 = []\n",
    "idx = 0\n",
    "for i in results_truths:\n",
    "            if i == results_predictions2[idx]:\n",
    "                accurate_predictions_indices2.append(idx)\n",
    "                prediction_status2.append(True)\n",
    "                accurate_predictions_count2 +=1\n",
    "            elif results_predictions2[idx] == 'Error':\n",
    "                prediction_status2.append('Error')\n",
    "            else:\n",
    "                prediction_status2.append(False)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution accuracy of the model 1:  0.46\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution accuracy of the model 1: \", accurate_predictions_count1/len(predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution accuracy of the model 2:  0.63\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution accuracy of the model 2: \", accurate_predictions_count2/len(predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing Evaluator to estimate the difficulty of SQL queries\n",
    "eval = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls_recon = json.load(open(DEV_DIR))\n",
    "parsed_queries_isrecon = [i['sql'] for i in sqls_recon]\n",
    "hardness_status_isrecon1 = [eval.eval_hardness(i) for i in parsed_queries_isrecon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium    0.51\n",
       "easy      0.19\n",
       "hard      0.15\n",
       "extra     0.15\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hardness_status_isrecon1).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SQL true</th>\n",
       "      <th>SQL pred 1</th>\n",
       "      <th>SQL pred 2</th>\n",
       "      <th>Hardness level</th>\n",
       "      <th>Prediction status 1</th>\n",
       "      <th>Prediction status 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT distinct papers.title from papers join ...</td>\n",
       "      <td>SELECT papers.title FROM authors JOIN papers O...</td>\n",
       "      <td>SELECT DISTINCT papers.title FROM papers JOIN ...</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT COUNT(distinct papers.title) from paper...</td>\n",
       "      <td>SELECT COUNT(*) FROM authors JOIN papers ON pa...</td>\n",
       "      <td>SELECT COUNT(*) FROM papers JOIN authors ON au...</td>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>select authors.firstname, authors.surname from...</td>\n",
       "      <td>SELECT DISTINCT COUNT(*), papers.authors FROM ...</td>\n",
       "      <td>SELECT authors.firstname FROM papers JOIN auth...</td>\n",
       "      <td>extra</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT count(distinct papers.article_id) from ...</td>\n",
       "      <td>SELECT COUNT(*) FROM authors JOIN papers ON pa...</td>\n",
       "      <td>SELECT COUNT(DISTINCT papers.title) FROM paper...</td>\n",
       "      <td>medium</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT papers.title from papers join authors o...</td>\n",
       "      <td>SELECT papers.title FROM authors JOIN papers O...</td>\n",
       "      <td>SELECT papers.title FROM papers JOIN authors O...</td>\n",
       "      <td>medium</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SQL true  \\\n",
       "0  SELECT distinct papers.title from papers join ...   \n",
       "1  SELECT COUNT(distinct papers.title) from paper...   \n",
       "2  select authors.firstname, authors.surname from...   \n",
       "3  SELECT count(distinct papers.article_id) from ...   \n",
       "4  SELECT papers.title from papers join authors o...   \n",
       "\n",
       "                                          SQL pred 1  \\\n",
       "0  SELECT papers.title FROM authors JOIN papers O...   \n",
       "1  SELECT COUNT(*) FROM authors JOIN papers ON pa...   \n",
       "2  SELECT DISTINCT COUNT(*), papers.authors FROM ...   \n",
       "3  SELECT COUNT(*) FROM authors JOIN papers ON pa...   \n",
       "4  SELECT papers.title FROM authors JOIN papers O...   \n",
       "\n",
       "                                          SQL pred 2 Hardness level  \\\n",
       "0  SELECT DISTINCT papers.title FROM papers JOIN ...         medium   \n",
       "1  SELECT COUNT(*) FROM papers JOIN authors ON au...         medium   \n",
       "2  SELECT authors.firstname FROM papers JOIN auth...          extra   \n",
       "3  SELECT COUNT(DISTINCT papers.title) FROM paper...         medium   \n",
       "4  SELECT papers.title FROM papers JOIN authors O...         medium   \n",
       "\n",
       "  Prediction status 1  Prediction status 2  \n",
       "0               False                 True  \n",
       "1               False                False  \n",
       "2               False                False  \n",
       "3                True                 True  \n",
       "4                True                 True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df = pd.DataFrame(data = {'SQL true': queries, 'SQL pred 1': predictions1, 'SQL pred 2': predictions2, 'Hardness level': hardness_status_isrecon1, 'Prediction status 1': prediction_status1, 'Prediction status 2': prediction_status2})\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hardness level</th>\n",
       "      <th>easy</th>\n",
       "      <th>extra</th>\n",
       "      <th>hard</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction status 1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>21.1</td>\n",
       "      <td>93.3</td>\n",
       "      <td>46.7</td>\n",
       "      <td>52.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>73.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>53.3</td>\n",
       "      <td>45.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error</th>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hardness level       easy  extra  hard  medium\n",
       "Prediction status 1                           \n",
       "False                21.1   93.3  46.7    52.9\n",
       "True                 73.7    6.7  53.3    45.1\n",
       "Error                 5.3    0.0   0.0     2.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of accurate predictions according to the hardness level of SQL\n",
    "pd.crosstab(index=evaluation_df['Prediction status 1'], columns=evaluation_df['Hardness level'],normalize='columns').round(3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hardness level</th>\n",
       "      <th>easy</th>\n",
       "      <th>extra</th>\n",
       "      <th>hard</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction status 2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>26.3</td>\n",
       "      <td>26.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>73.7</td>\n",
       "      <td>73.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hardness level       easy  extra  hard  medium\n",
       "Prediction status 2                           \n",
       "False                26.3   26.7  14.3    50.0\n",
       "True                 73.7   73.3  85.7    50.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of accurate predictions according to the hardness level of SQL\n",
    "pd.crosstab(index=evaluation_df['Prediction status 2'], columns=evaluation_df['Hardness level'],normalize='columns').round(3)*100"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54676cebd2e3594d56c4f6c64ad83f55d2fbe2f4f36d232b90b4a8856cfddab2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('my_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
