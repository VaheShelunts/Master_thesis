{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shelunts/thesis/TabularSemantingParsing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_inputs.parse_sql_one import get_schemas_from_json, Schema\n",
    "from preprocess_inputs.process_sql import get_sql\n",
    "from preprocess_inputs.get_tables import convert_fk_index, dump_db_json_schema\n",
    "import json\n",
    "import sqlite3\n",
    "from os.path import isfile, isdir, join, split, exists, splitext\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from src.eval.spider.evaluate import Evaluator\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "DATA_DIR = ROOT_DIR + '/data/isrecon/isrecon/isrecon.sqlite'\n",
    "SCHEMA_DIR = ROOT_DIR + '/data/spider/tables.json' \n",
    "SQL_DIR = ROOT_DIR + '/data/isrecon/isrecon/test_sql_pairs.sql'\n",
    "DEV_DIR = ROOT_DIR + '/data/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDS_DIR_ACADEMIC = ROOT_DIR +'/data/spider/dev_academic.json'\n",
    "LOG_DIR_ACADEMIC = ROOT_DIR + '/test_preds_outputs/inference_output2_academic.txt'\n",
    "PREDS_DIR_COLLEGE = ROOT_DIR + '/data/spider/dev_college_2.json'\n",
    "LOG_DIR_COLLEGE = ROOT_DIR + '/test_preds_outputs/inference_output8_on_college_placeholder.txt'\n",
    "PREDS_DIR_SINGER = ROOT_DIR + '/data/spider/dev_singer.json'\n",
    "LOG_DIR_SINGER = ROOT_DIR + '/test_preds_outputs/inference_output6_on_singer_placeholder.txt'\n",
    "PREDS_DIR_CAR = ROOT_DIR + '/data/spider/dev_car_1.json'\n",
    "LOG_DIR_CAR = ROOT_DIR + '/test_preds_outputs/inference_output9_on_cars_placeholder.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Academic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing Evaluator to estimate the difficulty of SQL queries\n",
    "eval = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing sqls and parsed sqls to estimate difficulty based on components\n",
    "sqls_academic = json.load(open(PREDS_DIR_ACADEMIC))\n",
    "queries_academic = [re.sub(\";\",\"\",i[\"query\"]) for i in sqls_academic]\n",
    "parsed_queries_academic = [i['sql'] for i in sqls_academic]\n",
    "hardness_status_academic = [eval.eval_hardness(i) for i in parsed_queries_academic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the file with wrong predictions as a look-up for mapping wrong predictions and doing some preprocessing\n",
    "with open(LOG_DIR_ACADEMIC) as preds_file_academic:\n",
    "    wrong_preds_output_academic = preds_file_academic.read().split('\\n')\n",
    "wrong_preds_academic = [re.sub(\"Target 0: b'\",'',i) for i in wrong_preds_output_academic if len(re.findall('^Target 0: b', i))>0]\n",
    "wrong_preds_academic = [re.sub(\"\\'\",\"\",i) for i in wrong_preds_academic]\n",
    "wrong_preds_academic = [re.sub(\"'\",\"\",i) for i in wrong_preds_academic]\n",
    "pred_status_academic = [False if i in wrong_preds_academic else True for i in queries_academic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT t3.paperid FROM writes AS t3 JOIN author AS t2 ON t3.authorid  =  t2.authorid JOIN writes AS t4 ON t4.paperid  =  t3.paperid JOIN author AS t1 ON t4.authorid  =  t1.authorid WHERE t2.authorname  =  \"Peter Mertens\" AND t1.authorname  =  \"Dina Barbian\"\n",
      "SELECT DISTINCT t3.paperid FROM writes AS t3 JOIN author AS t2 ON t3.authorid  =  t2.authorid JOIN writes AS t4 ON t4.paperid  =  t3.paperid JOIN author AS t1 ON t4.authorid  =  t1.authorid WHERE t2.authorname  =  \"Peter Mertens\" AND t1.authorname  =  \"Dina Barbian\"\n",
      "SELECT DISTINCT t3.paperid FROM writes AS t3 JOIN author AS t2 ON t3.authorid  =  t2.authorid JOIN writes AS t4 ON t4.paperid  =  t3.paperid JOIN author AS t1 ON t4.authorid  =  t1.authorid WHERE t2.authorname  =  \"Peter Mertens\" AND t1.authorname  =  \"Dina Barbian\"\n",
      "SELECT DISTINCT t3.paperid FROM writes AS t3 JOIN author AS t2 ON t3.authorid  =  t2.authorid JOIN writes AS t4 ON t4.paperid  =  t3.paperid JOIN author AS t1 ON t4.authorid  =  t1.authorid WHERE t2.authorname  =  \"Peter Mertens\" AND t1.authorname  =  \"Dina Barbian\"\n",
      "SELECT DISTINCT t3.paperid FROM writes AS t3 JOIN author AS t2 ON t3.authorid  =  t2.authorid JOIN writes AS t4 ON t4.paperid  =  t3.paperid JOIN author AS t1 ON t4.authorid  =  t1.authorid WHERE t2.authorname  =  \"Peter Mertens\" AND t1.authorname  =  \"Dina Barbian\"\n"
     ]
    }
   ],
   "source": [
    "for i in wrong_preds_academic[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of right and wrong predictions\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    0.973333\n",
       "True     0.026667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of right and wrong predictions\\n\")\n",
    "pd.Series(pred_status_academic).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SQL true</th>\n",
       "      <th>Hardness level</th>\n",
       "      <th>Prediction status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT DISTINCT t3.paperid FROM writes AS t3 J...</td>\n",
       "      <td>extra</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT DISTINCT t3.paperid FROM writes AS t3 J...</td>\n",
       "      <td>extra</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT DISTINCT t3.paperid FROM writes AS t3 J...</td>\n",
       "      <td>extra</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT DISTINCT t3.paperid FROM writes AS t3 J...</td>\n",
       "      <td>extra</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT DISTINCT t3.paperid FROM writes AS t3 J...</td>\n",
       "      <td>extra</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SQL true Hardness level  \\\n",
       "0  SELECT DISTINCT t3.paperid FROM writes AS t3 J...          extra   \n",
       "1  SELECT DISTINCT t3.paperid FROM writes AS t3 J...          extra   \n",
       "2  SELECT DISTINCT t3.paperid FROM writes AS t3 J...          extra   \n",
       "3  SELECT DISTINCT t3.paperid FROM writes AS t3 J...          extra   \n",
       "4  SELECT DISTINCT t3.paperid FROM writes AS t3 J...          extra   \n",
       "\n",
       "   Prediction status  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df_academic = pd.DataFrame(data = {'SQL true': queries_academic, 'Hardness level': hardness_status_academic, 'Prediction status': pred_status_academic})\n",
    "evaluation_df_academic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of right and wrong predictions per SQL hardness groups\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hardness level</th>\n",
       "      <th>easy</th>\n",
       "      <th>extra</th>\n",
       "      <th>hard</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>62.1</td>\n",
       "      <td>99.8</td>\n",
       "      <td>99.3</td>\n",
       "      <td>93.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>37.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hardness level     easy  extra  hard  medium\n",
       "Prediction status                           \n",
       "False              62.1   99.8  99.3    93.7\n",
       "True               37.9    0.2   0.7     6.3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of right and wrong predictions per SQL hardness groups\\n\")\n",
    "pd.crosstab(index=evaluation_df_academic['Prediction status'], columns=evaluation_df_academic['Hardness level'],normalize='columns').round(3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: College predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls = json.load(open(PREDS_DIR_COLLEGE))\n",
    "queries_college = [re.sub(\";\",\"\",i[\"query\"]) for i in sqls]\n",
    "parsed_queries_college = [i['sql'] for i in sqls]\n",
    "hardness_status_college = [eval.eval_hardness(i) for i in parsed_queries_college]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOG_DIR_COLLEGE,'r') as preds_file_college:\n",
    "    wrong_preds_output_college = preds_file_college.read().split('\\n')\n",
    "wrong_preds_college = [re.sub(\"^Target 0: b'\",'',i) for i in wrong_preds_output_college if len(re.findall('^Target 0: b', i))>0]\n",
    "wrong_preds_college = [re.findall(\"SELECT.+\",i) for i in wrong_preds_college if len(re.findall(\"SELECT.+'\",i))>0]\n",
    "wrong_preds_college = [k for i in wrong_preds_college for k in i]\n",
    "wrong_preds_college = [i.rstrip(\"'\") for i in wrong_preds_college]\n",
    "wrong_preds_college = [i.rstrip('\"') for i in wrong_preds_college]\n",
    "wrong_preds_college = [i.strip(\"'\") for i in wrong_preds_college]\n",
    "pred_status_college = [False if i in wrong_preds_college else True for i in queries_college]\n",
    "#wrong_preds_college = [re.sub(\"42000'\",\"42000\",i) for i in wrong_preds_college]\n",
    "#wrong_preds_college = [re.sub(\"teaches'\",\"teaches\",i) for i in wrong_preds_college]\n",
    "evaluation_df_college = pd.DataFrame(data = {'SQL true': queries_college, 'Hardness level': hardness_status_college, 'Prediction status': pred_status_college})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT count(*) FROM classroom WHERE building != 'Lamberton\n",
      "SELECT name FROM student WHERE dept_name  =  'History' ORDER BY tot_cred DESC LIMIT 1\n",
      "SELECT name FROM student WHERE dept_name  =  'History' ORDER BY tot_cred DESC LIMIT 1\n",
      "SELECT count(DISTINCT course_id) FROM course WHERE dept_name  =  'Physics\n",
      "SELECT count(DISTINCT course_id) FROM course WHERE dept_name  =  'Physics\n"
     ]
    }
   ],
   "source": [
    "for i in wrong_preds_college[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of right and wrong predictions per SQL hardness groups\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hardness level</th>\n",
       "      <th>easy</th>\n",
       "      <th>extra</th>\n",
       "      <th>hard</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>33.3</td>\n",
       "      <td>78.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>66.7</td>\n",
       "      <td>21.4</td>\n",
       "      <td>24.2</td>\n",
       "      <td>53.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hardness level     easy  extra  hard  medium\n",
       "Prediction status                           \n",
       "False              33.3   78.6  75.8    46.2\n",
       "True               66.7   21.4  24.2    53.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of right and wrong predictions per SQL hardness groups\\n\")\n",
    "pd.crosstab(index=evaluation_df_college['Prediction status'], columns=evaluation_df_college['Hardness level'],normalize='columns').round(3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Singer predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls_singer = json.load(open(PREDS_DIR_SINGER))\n",
    "queries_singer = [i[\"query\"] for i in sqls_singer]\n",
    "queries_singer = [re.sub(\";\",\"\",i[\"query\"]) for i in sqls_singer]\n",
    "parsed_queries_singer = [i['sql'] for i in sqls_singer]\n",
    "hardness_status_singer = [eval.eval_hardness(i) for i in parsed_queries_singer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOG_DIR_SINGER) as preds_file_singer:\n",
    "    wrong_preds_output_singer = preds_file_singer.read().split('\\n')\n",
    "wrong_preds_singer = [re.sub('^Target 0: b','',i) for i in wrong_preds_output_singer if len(re.findall('^Target 0: b', i))>0]\n",
    "wrong_preds_singer = [i.rstrip(\"'\") for i in wrong_preds_singer]\n",
    "wrong_preds_singer = [i.strip('\"') for i in wrong_preds_singer]\n",
    "wrong_preds_singer = [i.lstrip(\"'\") for i in wrong_preds_singer]\n",
    "pred_status_singer = [False if i in wrong_preds_singer else True for i in queries_singer]\n",
    "evaluation_df_singer = pd.DataFrame(data = {'SQL true': queries_singer, 'Hardness level': hardness_status_singer, 'Prediction status': pred_status_singer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT avg(age) ,  min(age) ,  max(age) FROM singer WHERE country  =  'France'\n",
      "select max(capacity), average from stadium\n",
      "SELECT name ,  capacity FROM stadium ORDER BY average DESC LIMIT 1\n",
      "select count(*) from concert where stadium_id = (select stadium_id from stadium order by capacity desc limit 1)\n",
      "select count(*) from concert where stadium_id = (select stadium_id from stadium order by capacity desc limit 1)\n"
     ]
    }
   ],
   "source": [
    "for i in wrong_preds_singer[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium    0.53\n",
       "hard      0.29\n",
       "easy      0.09\n",
       "extra     0.09\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hardness_status_singer).value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of right and wrong predictions per SQL hardness groups\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hardness level</th>\n",
       "      <th>easy</th>\n",
       "      <th>extra</th>\n",
       "      <th>hard</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.6</td>\n",
       "      <td>79.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hardness level      easy  extra  hard  medium\n",
       "Prediction status                            \n",
       "False                0.0    0.0  15.4    20.8\n",
       "True               100.0  100.0  84.6    79.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of right and wrong predictions per SQL hardness groups\\n\")\n",
    "pd.crosstab(index=evaluation_df_singer['Prediction status'], columns=evaluation_df_singer['Hardness level'],normalize='columns').round(3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Car predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls = json.load(open(PREDS_DIR_CAR))\n",
    "queries_cars = [re.sub(\";\",\"\",i[\"query\"]) for i in sqls]\n",
    "parsed_queries_cars = [i['sql'] for i in sqls]\n",
    "hardness_status_cars = [eval.eval_hardness(i) for i in parsed_queries_cars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium    0.35\n",
       "extra     0.28\n",
       "easy      0.20\n",
       "hard      0.17\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hardness_status_cars).value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOG_DIR_CAR) as preds_file_cars:\n",
    "    wrong_preds_output_cars = preds_file_cars.read().split('\\n')\n",
    "wrong_preds_cars = [re.sub('^Target 0: b','',i) for i in wrong_preds_output_cars if len(re.findall('^Target 0: b', i))>0]\n",
    "wrong_preds_cars = [i.replace(\"\\\\\", \"\") for i in wrong_preds_cars]\n",
    "wrong_preds_cars = [i.rstrip(\"'\") for i in wrong_preds_cars]\n",
    "wrong_preds_cars = [i.strip('\"') for i in wrong_preds_cars]\n",
    "wrong_preds_cars = [i.lstrip(\"'\") for i in wrong_preds_cars]\n",
    "pred_status_cars = [False if i in wrong_preds_cars else True for i in queries_cars]\n",
    "#wrong_preds_college = [re.sub(\"42000'\",\"42000\",i) for i in wrong_preds_college]\n",
    "#wrong_preds_college = [re.sub(\"teaches'\",\"teaches\",i) for i in wrong_preds_college]\n",
    "evaluation_df_cars = pd.DataFrame(data = {'SQL true': queries_cars, 'Hardness level': hardness_status_cars, 'Prediction status': pred_status_cars})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT T1.ContId ,  T1.Continent ,  count(*) FROM CONTINENTS AS T1 JOIN COUNTRIES AS T2 ON T1.ContId  =  T2.Continent GROUP BY T1.ContId\n",
      "SELECT T1.ContId ,  T1.Continent ,  count(*) FROM CONTINENTS AS T1 JOIN COUNTRIES AS T2 ON T1.ContId  =  T2.Continent GROUP BY T1.ContId\n",
      "SELECT T1.FullName ,  T1.Id ,  count(*) FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id  =  T2.Maker GROUP BY T1.Id\n",
      "SELECT T1.Model FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId  =  T2.Id ORDER BY T2.horsepower ASC LIMIT 1\n",
      "SELECT T1.Model FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId  =  T2.Id ORDER BY T2.horsepower ASC LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "for i in wrong_preds_cars[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium    0.35\n",
       "extra     0.28\n",
       "easy      0.20\n",
       "hard      0.17\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hardness_status_cars).value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.74\n",
       "True     0.26\n",
       "Name: Prediction status, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df_cars['Prediction status'].value_counts(normalize=True).round(2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54676cebd2e3594d56c4f6c64ad83f55d2fbe2f4f36d232b90b4a8856cfddab2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('my_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
